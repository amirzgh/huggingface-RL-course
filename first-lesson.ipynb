{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Reinforcement learning course\n",
    "this is a personal note from this course  \n",
    "all credits goes to higging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is reinforcement learning in a nutshell\n",
    "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL process can be summed up in these points\n",
    "- our agent receives a **state** $S_0$ from the environment\n",
    "- based on $S_0$ aget takes **action** $A_0$\n",
    "- the environment goes to a **new state** $S_1$\n",
    "- the environment give the agent the **reward** $R_1$\n",
    "\n",
    "This **RL** loops results in a sequence of **states, actions, rewards**  \n",
    "The task in RL is to maximise its cumulative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation space\n",
    "**State**: Complete description of the state with no hidden information (chess)  \n",
    "**Observation**: Partional description of the state of the world (mario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "set of all possible actions in environment. can be **discerete** or **continues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards and Discounts\n",
    "\n",
    "rewards is the feedback to our module and by this our agent finds out if the taken action was good or bad.  \n",
    "\n",
    "the cumulative rewarda at time step $t$ can be written as bellow:  \n",
    "$$\n",
    "    R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1} = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + \\dots\n",
    "$$\n",
    "\n",
    "1. as we care about future rewards more than early rewards we use discount rate $0 < \\gamma < 1$.\n",
    "\n",
    "- the larger the gamma($\\gamma$), the smaller the discount. This means our agent cares more about the long-term reward.\n",
    "- On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n",
    "\n",
    "2. Then, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat (in the example of the mouse and cheese) gets closer to us, so the future reward is less and less likely to happen.\n",
    "\n",
    "our discounted expected cumulative reward is:\n",
    "\n",
    "$$\n",
    "    R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^kr_{t+k+1} = r_{t+1} + \\gamma r_{t+2} + \\gamma^2r_{t+3} + \\gamma^3r_{t+4} + \\dots\n",
    "$$\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation\n",
    "\n",
    "Exploration: trying random actions in order to find more information about the environment.  \n",
    "Expoitation: using known information to maximise the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Policy ${\\pi}$ and Two approaches in RL\n",
    "\n",
    "The Policy ${\\pi}$ acts as our Agent brain and outputs the action given the state.  \n",
    "$$\n",
    "    action = \\pi(state)\n",
    "$$\n",
    "\n",
    "our goal in RL in to find $\\pi^*$ in order to maximise the expected return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is two approaches to train our model(agent)\n",
    "- direct: teach the agent to take actions given the state **(Policy based)**  \n",
    "- indirect: teach the agent to learn the best action in any given state **(Value based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy based approach\n",
    "\n",
    "we learn a policy function directly. This function will define a mapping from each state to the best corresponding action.\n",
    "\n",
    "Policy based method can either be **deterministic** or **stochastic**\n",
    "\n",
    "- **Deterministic**: Policy at a given state will always return the same action.  \n",
    "$$a = \\pi(state)$$\n",
    "\n",
    "- **Stochastic**: output the probability distributino over the set of actions.  \n",
    "$$ \\pi(a | s) = P[A|s]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value based approach\n",
    "\n",
    "In value-based methods, instead of learning a policy function, we learn a value function that maps a state to the expected value of being at that state.  \n",
    "“Act according to our policy” just means that our policy is “going to the state with the highest value”.\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots \\mid S_t = s\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
